{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM Shakespeare (by word)",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ra2yama/shakespeare-generator/blob/master/LSTM_Shakespeare_(by_word).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8sNQoV5aMk4B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-MHgYRuWxkQ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('hamlet.txt', 'https://gist.githubusercontent.com/provpup/2fc41686eab7400b796b/raw/b575bd01a58494dfddc1d6429ef0167e709abf9b/hamlet.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v2h928GRtC_i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N_I43XL-rXf9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def reconstruct(list):\n",
        "  return \" \".join(list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ym0sCgBIMw8Y",
        "colab_type": "code",
        "outputId": "9b05dd7d-e2cd-4738-b8a0-002245404a34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "text = open(path_to_file).read()\n",
        "print ('Length of text: {} characters'.format(len(text)))\n",
        "text = text[844:-1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 191726 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_jMbnBj8E9sV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words = re.findall(r'\\S+|\\n',text)\n",
        "available_words = list(set(words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XGJBt4aaFm9N",
        "colab_type": "code",
        "outputId": "fe9d45d1-89e1-4c03-ebc9-d84be0d59f6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(available_words))\n",
        "# outputs basically"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lemo049CsRAB",
        "colab_type": "code",
        "outputId": "284ef722-fc96-4d2c-e11c-9a9085e499ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "67 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HF3OP2EKsgcO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J9iB1V4_sj9Q",
        "colab_type": "code",
        "outputId": "42d6c961-6303-4b31-afec-58436135392e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('{:6s} ---> {:4d}'.format(repr(char), char2idx[char]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'\\n'   --->    0\n",
            "'!'    --->    2\n",
            "' '    --->    1\n",
            "'\"'    --->    3\n",
            "\"'\"    --->    5\n",
            "'&'    --->    4\n",
            "')'    --->    7\n",
            "'('    --->    6\n",
            "'-'    --->    9\n",
            "','    --->    8\n",
            "'.'    --->   10\n",
            "'1'    --->   11\n",
            "';'    --->   13\n",
            "':'    --->   12\n",
            "'?'    --->   14\n",
            "'A'    --->   15\n",
            "'C'    --->   17\n",
            "'B'    --->   16\n",
            "'E'    --->   19\n",
            "'D'    --->   18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aszSs_BntBIu",
        "colab_type": "code",
        "outputId": "bddb84b4-3fe5-411d-a224-0b34624d9374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "chunks = tf.data.Dataset.from_tensor_slices(text_as_int).batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in chunks.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'SCENE.- Elsinore.\\n\\n\\nACT I. Scene I.\\nElsinore. A platform before the Castle.\\n\\nEnter two Sentinels-[fir'\n",
            "'st,] Francisco, [who paces up and down\\nat his post; then] Bernardo, [who approaches him].\\n\\n  Ber. Who'\n",
            "\"'s there.?\\n  Fran. Nay, answer me. Stand and unfold yourself.\\n  Ber. Long live the King!\\n  Fran. Bern\"\n",
            "\"ardo?\\n  Ber. He.\\n  Fran. You come most carefully upon your hour.\\n  Ber. 'Tis now struck twelve. Get t\"\n",
            "\"hee to bed, Francisco.\\n  Fran. For this relief much thanks. 'Tis bitter cold,\\n    And I am sick at he\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SIDnlkhcte3X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = chunks.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R1TO_610sxDE",
        "colab_type": "code",
        "outputId": "4307c4a1-4107-4617-df20-b0110ab38089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print ('{} ---- characters mapped to int ---- > {}'.format(text[:13], text_as_int[:13]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SCENE.- Elsin ---- characters mapped to int ---- > [33 17 19 28 19 10  9  1 19 52 59 49 54]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Sv0MBc14uFxN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Batch size \n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences, \n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SCnkMMA4uRa2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, units):\n",
        "    super(Model, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    if tf.test.is_gpu_available():\n",
        "      self.gru = tf.keras.layers.CuDNNGRU(self.units, \n",
        "                                          return_sequences=True, \n",
        "                                          recurrent_initializer='glorot_uniform',\n",
        "                                          stateful=True)\n",
        "    else:\n",
        "      self.gru = tf.keras.layers.GRU(self.units, \n",
        "                                     return_sequences=True, \n",
        "                                     recurrent_activation='sigmoid', \n",
        "                                     recurrent_initializer='glorot_uniform', \n",
        "                                     stateful=True)\n",
        "\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "  def call(self, x):\n",
        "    embedding = self.embedding(x)\n",
        "    \n",
        "    # output at every time step\n",
        "    # output shape == (batch_size, seq_length, hidden_size) \n",
        "    output = self.gru(embedding)\n",
        "    \n",
        "    # The dense layer will output predictions for every time_steps(seq_length)\n",
        "    # output shape after the dense layer == (seq_length * batch_size, vocab_size)\n",
        "    prediction = self.fc(output)\n",
        "    \n",
        "    # states will be used to pass at every step to the model while training\n",
        "    return prediction\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1LFml8ZXuTrh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension \n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "units = 1024\n",
        "\n",
        "model = Model(vocab_size, embedding_dim, units)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tnr_5hzIuvP7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "60VL7tiJy-y9",
        "colab_type": "code",
        "outputId": "10fe6bd1-9d04-4804-b62a-24b1c35b50bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f4d5b9c4ad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "metadata": {
        "id": "JSm_TyMHuzQ1",
        "colab_type": "code",
        "outputId": "fe48ebef-1030-48a8-d86b-fa879eeb42f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    # initializing the hidden state at the start of every epoch\n",
        "    # initally hidden is None\n",
        "    hidden = model.reset_states()\n",
        "    \n",
        "    for (batch, (inp, target)) in enumerate(dataset):\n",
        "          with tf.GradientTape() as tape:\n",
        "              # feeding the hidden state back into the model\n",
        "              # This is the interesting step\n",
        "              predictions = model(inp)\n",
        "              loss = loss_function(target, predictions)\n",
        "              \n",
        "          grads = tape.gradient(loss, model.variables)\n",
        "          optimizer.apply_gradients(zip(grads, model.variables))\n",
        "\n",
        "          if batch % 100 == 0:\n",
        "              print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
        "                                                            batch,\n",
        "                                                            loss))\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      model.save_weights(checkpoint_prefix)\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.1665\n",
            "Epoch 1 Loss 0.2520\n",
            "Time taken for 1 epoch 4.32525992393 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.1545\n",
            "Epoch 2 Loss 0.2430\n",
            "Time taken for 1 epoch 4.16333794594 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.1540\n",
            "Epoch 3 Loss 0.2427\n",
            "Time taken for 1 epoch 4.16568112373 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.1419\n",
            "Epoch 4 Loss 0.2096\n",
            "Time taken for 1 epoch 4.17448401451 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1362\n",
            "Epoch 5 Loss 0.2252\n",
            "Time taken for 1 epoch 4.22853589058 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1455\n",
            "Epoch 6 Loss 0.2182\n",
            "Time taken for 1 epoch 4.17860889435 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.1380\n",
            "Epoch 7 Loss 0.2175\n",
            "Time taken for 1 epoch 4.21463298798 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1263\n",
            "Epoch 8 Loss 0.2094\n",
            "Time taken for 1 epoch 4.18945598602 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1363\n",
            "Epoch 9 Loss 0.2220\n",
            "Time taken for 1 epoch 4.18125987053 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1308\n",
            "Epoch 10 Loss 0.2006\n",
            "Time taken for 1 epoch 4.21829009056 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BUotEb2I4PTE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "outputId": "94c967b0-800c-4b96-8989-25cd46f07978"
      },
      "cell_type": "code",
      "source": [
        "#5\n",
        "!ls {checkpoint_dir}\n",
        "!cd {checkpoint_dir}\n",
        "!cat ./training_checkpoints/ch"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint  ckpt.data-00000-of-00001  ckpt.index\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-151-92cf3a9d9248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'ls {checkpoint_dir}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'cd {checkpoint_dir}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'cat ./training_checkpoints/ckpt.index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/colab/_shell.pyc\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mShell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/colab/_system_commands.pyc\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd)\u001b[0m\n\u001b[1;32m    434\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m   result = _run_command(\n\u001b[0;32m--> 436\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    437\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/colab/_system_commands.pyc\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/colab/_system_commands.pyc\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    220\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/colab/_system_commands.pyc\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0moutput_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0mraw_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_PTY_READ_MAX_BYTES_FOR_TEST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m       \u001b[0mdecoded_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python2.7/codecs.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf8' codec can't decode byte 0x93 in position 45: invalid start byte"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "xbpj8Ff-vt5m",
        "colab_type": "code",
        "outputId": "9d6b1a1d-eb6d-4786-a4b8-dd8ffe1d871c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "model.build(tf.TensorShape([BATCH_SIZE, seq_length]))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      multiple                  17152     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_7 (CuDNNGRU)       multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              multiple                  68675     \n",
            "=================================================================\n",
            "Total params: 4,024,131\n",
            "Trainable params: 4,024,131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cziRwqZWuZXu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Using adam optimizer with default arguments\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "# Using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
        "def loss_function(real, preds):\n",
        "    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "syL2HoZVuqMP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U4eINd98M40W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(words[:10000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xr7iq2zqpVD-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QNImZW5rrydA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(reconstruct(words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rjOr-WS1thfi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VfloKPC8tzfx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nyBzlJQtx2uq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#PREDICTING TEXT\n",
        "model.save_weights(checkpoint_prefix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZBsh-VcayRYX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6qgi2XUGx9-X",
        "colab_type": "code",
        "outputId": "65351113-7a5f-4d33-cb02-b0089dc593e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls {checkpoint_dir}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint  ckpt.data-00000-of-00001  ckpt.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VHGHnGaLyE30",
        "colab_type": "code",
        "outputId": "75b61210-807c-4f67-f5ab-b4b12a7229ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "u'./training_checkpoints/ckpt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "metadata": {
        "id": "rihKsfYTySjB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Evaluation step (generating text using the learned model)\n",
        "\n",
        "# Number of characters to generate\n",
        "num_generate = 5000\n",
        "\n",
        "# You can change the start string to experiment\n",
        "start_string = 'S'\n",
        "\n",
        "# Converting our start string to numbers (vectorizing) \n",
        "input_eval = [char2idx[s] for s in start_string]\n",
        "input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "# Empty string to store our results\n",
        "text_generated = []\n",
        "\n",
        "# Low temperatures results in more predictable text.\n",
        "# Higher temperatures results in more surprising text.\n",
        "# Experiment to find the best setting.\n",
        "temperature = 1.0\n",
        "\n",
        "out_model = Model(vocab_size, embedding_dim, units)\n",
        "\n",
        "out_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "out_model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SfIyZ1rEyVuc",
        "colab_type": "code",
        "outputId": "ac0d06f0-742c-4fc6-e2f0-ade9a8716bb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1785
        }
      },
      "cell_type": "code",
      "source": [
        "# Evaluation loop.\n",
        "\n",
        "# Here batch size == 1\n",
        "out_model.reset_states()\n",
        "for i in range(num_generate):\n",
        "    predictions = out_model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a multinomial distribution to predict the word returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "    \n",
        "    # We pass the predicted word as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    \n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "print (start_string + ''.join(text_generated))"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shoo more!\n",
            "    These words like daggers enter in mine ears.\n",
            "    No  I leave think of this\n",
            "    HApirt'd from a handsaw.\n",
            "  \n",
            "                    Enter Queen.\n",
            "\n",
            "    How now?\n",
            "  All. Give fire.  \n",
            "  Ham. No, by my former llus fan\n",
            "    The inward service. We rat be, unless she drown'd herself in his chiefes in such mett.\n",
            "    Let me not ducattlemark.\n",
            "      Do not ases be time,\n",
            "     The here a- but lenes my lord.\n",
            "  Ham. I cannot dount marrie own is your offence in't?\n",
            "  Ham. No, no! They do but jest, poison in jest; no offence i' th'\n",
            "    world.\n",
            "  King. What, Gertrude, you shall be dry\n",
            "    again.\n",
            "  Ros. I understand you not, meand many more of the same bevy that I live the okn my lord; I have.\n",
            "  Pol. How doe you must fie, faith, not if you have of life be windrous and toreen'd currult.\n",
            "    Do you consent we shall acquaint him with it,\n",
            "    As not faith. Sir, his hide is so tann'd with his trade that 'a will\n",
            "    knew for. I was this seal'd and done\n",
            "    Than tre I'll loosifis baw.\n",
            "  Ham. I will tell you.\n",
            "  Ham. But to the Queen?\n",
            "  King. She sould in the grasple and:\n",
            "    About and as many  \n",
            "    As of a father; for leave to do\n",
            "    And wat a gentleman would come.\n",
            "  Mar. How is't, my noble lord?\n",
            "  Hor. What news, my lord- How now? What ndist as man may move thee,\n",
            "    In very pround.\n",
            "  Ber. He is deceive to know that,\n",
            "    Before You visit hie, with all his cause wherein not sifpers and miniscove of the play\n",
            "    Let his queen mother all alone entreat him lord.\n",
            "  Ham. I could interpret between you and your lofe your tellow.                          [Lifts up the arras and sees Polonius.]\n",
            "    Thou wretched, rash, intrud follow to make the judgment away.\n",
            "    What this show me awsir.\n",
            "  Osr. I know you go bose vouchs; for the players make your  han Palonius's. There's something in his soul\n",
            "    O'erm.\n",
            "  King. Who shall stay the mornins and this\n",
            "    (For lutt not be.'The moubt thou to' that of Entland dead\n",
            "      When second husband kisses me in bed.\n",
            "    King. I do believe you think what now you sends             [Horcument  if't be hunds there. Let all that is Should be,\n",
            "    More than in words?\n",
            "  Laer. What did you enuch and minister.\n",
            "    I will bestow him, and will answer well\n",
            "    The news is not true ne't.\n",
            "    Now piese us with as Away, I'll his visage wager\n",
            "    To dream: us towart and will in the imputation lapl keaghter.- My lord,\n",
            "    Do not know are they are hath us; o pastime?\n",
            "    To pay five dunn and lobly than the body of continency and advice,\n",
            "    Shall tell appen. What is the matter, you are keen.\n",
            "  Ham. It was about to speak, and with no addition\n",
            "    We o'errated be and is a thing?\n",
            "  Pol. At such a die- 'Tis a knavish piece of\n",
            "    whom his aught of quiets wildingl me sis lawless fit\n",
            "    Behind the arras hearing something\n",
            "    misther aid.\n",
            "    Marcellus?\n",
            "  Mar. The can in deep propase,\n",
            "    Which now, to claim my very would speak with me?\n",
            "  Ham. Out of this fellow suit. And y, on Polonius.\n",
            "\n",
            "Enter Queen. and Hamlet.\n",
            "  Hor. He waxes desperate with imagination.\n",
            "  Mar. Let's would you were so honcy. Oug gie of itself?\n",
            "    We'll put the matter! My lord, I do not know,\n",
            "    But truly I do fear it.\n",
            "  Pol. What's his courfesties to your father's death,\n",
            "    Where it be so and wife, and the survivor boubs,\n",
            "    And borrowing does was there;\n",
            "    or aften Why doys it a funeral and most unnatural murther.\n",
            "  Ham. My lord, I have.\n",
            "  Pol. God b' wi' ye, fare ye well!\n",
            "  Rey. Good my lord!                                                                        Exeunt Ghost and Hamlet.\n",
            "\n",
            "  Ham. Why, it be thou and soory. The funeral bak'd may as wick at earth are will it yield to Norway or the Pole\n",
            "    A ranker rate, sweet do me so far him to you, at sent to see you well.\n",
            "    Horatio!- or I do forget myself.\n",
            "  Hor. The same, my lord, and your pald\n",
            "    pocy of his further det of this mother, what's the matter?\n",
            "  Queen. Hamlet, thou hast clower my hand more than the satinigh  \n",
            "    And could of men distinguish, her election\n",
            "    Had defence?\n",
            "    And these cenmark's criad o'er the world's dife.\n",
            "    Cauld weavou. For thou thinefore bard like\n",
            "    fill he tell him his commandment is fulfill'd\n",
            "    That live and free against the Polack,\n",
            "    Which is not tomb enough and continent\n",
            "    To hide the slain? O, from this too this?\n",
            "  Mar. My lord, upon the platform where.- King. I pray you.     Pos. Madam, it\n",
            "    speak. 'Sblood, do you think I am easier to be play'd on bountas sumbits; young marry with his statutes, his recognizances, his\n",
            "    fines, his double vouchers, know, to diviou at offece difalers nither one\n",
            "    Unto England is and greatly to find quarrel in a straw\n",
            "    When honour's at the stake. How stand I then,\n",
            "    That have after the play be then to be cause he will by no means spirit!\n",
            "    When the blood burns, how prodigable may steen\n",
            "    My truth is hid, though it were hit indeed\n",
            "    The ampassed Polsomer vewhe with vouch is in a celement our precouse,\n",
            "      To Denmarked er not speak. So mine own. Besides, to be\n",
            "    demanded of a sponge, what reply is the \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-07jdlED63i0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "9b0c872c-f110-4c5d-9471-aaf1a36ca69c"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-162-0732184b68d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'my_test_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1075\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n\u001b[0m\u001b[1;32m   1078\u001b[0m                          'graph before calling run().')\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The Session graph is empty.  Add operations to the graph before calling run()."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Qc1glnzLAjYZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}